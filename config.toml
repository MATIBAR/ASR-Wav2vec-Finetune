[meta]
name = "ASR"
pretrained_path = "jonatasgrosman/wav2vec2-large-xlsr-53-italian" # <-- Adjust if using a different model ("jonatasgrosman/wav2vec2-large-xlsr-53-italian")
seed = 42
epochs = 20 # <-- Consider adjusting based on training performance
save_dir = "saved/"
gradient_accumulation_steps = 2
use_amp = false # <-- Enable if your hardware supports AMP and you want faster training
device_ids = "0" # <-- Adjust based on available GPUs (e.g., "0,1" for two GPUs)
sr = 44100 # <-- Ensure this matches your audio files' sample rate
max_clip_grad_norm = 5.0

[special_tokens]
bos_token = "<bos>"
eos_token = "<eos>"
unk_token = "<unk>"
pad_token = "<pad>"
prolungamento_token = "<PROLUNGAMENTO>"
pausa_token = "<PAUSA>"
city_name_token = "Cityname"
address_name_token = "Addressname"
name_token = "Name"
#AGGIUNGERE QUA I TAG USATI PER LE NOSTRE ANNOTAZIONI, COME TOKEN SPECIALI (<PROLUNGAMENTO>, <PAUSA> ... anche Cityname, Addressname, Name ?)

[huggingface]
push_to_hub = false # Ensure this is set to false to disable automatic pushing
push_every_validation_step = false
overwrite_output_dir = false
blocking = false

    [huggingface.args]
    local_dir = "huggingface-hub" # Local directory for saving models, won't be pushed to the cloud
    use_auth_token = false # No need for an auth token if not pushing to the hub
    clone_from = "" # No cloning from a repo since push is disabled

[train_dataset]
path = "base.base_dataset.BaseDataset"
    [train_dataset.args]
    path = "/path/to/your/train_dataset" # <-- Replace with your training data path
    preload_data = false
    delimiter = "|"
    nb_workers = 16

    [train_dataset.dataloader]
    batch_size = 8
    num_workers = 16
    pin_memory = true
    drop_last = true

    [train_dataset.sampler]
    shuffle = true
    drop_last = true

[val_dataset]
path = "base.base_dataset.BaseDataset"
    [val_dataset.args]
    path = "/path/to/your/val_dataset" # <-- Replace with your validation data path
    preload_data = false
    delimiter = "|"
    nb_workers = 16

    [val_dataset.dataloader]
    batch_size = 1
    num_workers = 4

    [val_dataset.sampler]
    shuffle = false
    drop_last = false

[optimizer]
lr = 1e-6 # <-- Consider adjusting based on training performance

[scheduler]
max_lr = 5e-4 # <-- Consider adjusting based on training performance

[trainer]
path = "trainer.trainer.Trainer"
    [trainer.args]
    validation_interval = 500 # <-- Adjust based on how often you want to validate
    save_max_metric_score = false # <-- Set to true to save only the best models
